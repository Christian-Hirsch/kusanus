{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem, auf https://github.com/m2dsupsdlclass/lectures-labs/blob/master/labs/06_seq2seq/french_numbers.py basierenden, Notebook bauen wir ein character-basiertes Modell zur Texterzeugung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from french_numbers import generate_translations, to_french_phrase\n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.layers import Embedding, Dropout, LSTM, GRU, Dense\n",
    "from tensorflow.contrib.keras.python.keras.optimizers import Adam\n",
    "from tensorflow.contrib.keras.python.keras.preprocessing import sequence\n",
    "from tensorflow.contrib.keras.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "#seed for randomness\n",
    "SEED =42\n",
    "\n",
    "#maximum sentence length\n",
    "MAXLEN = 20\n",
    "\n",
    "#learning rate \n",
    "LR = 1e-3\n",
    "\n",
    "#maximum number of words\n",
    "MAX_NB_WORDS = 50\n",
    "\n",
    "#BATCH SIZE\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir betrachten einige Beispielzahlen... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 vingt et un\n",
      "80 quatre vingts\n",
      "81 quatre vingt un\n",
      "300 trois cents\n",
      "213 deux cent treize\n",
      "1100 mille cent\n",
      "1201 mille deux cent un\n",
      "301000 trois cent un mille\n",
      "80080 quatre vingt mille quatre vingts\n"
     ]
    }
   ],
   "source": [
    "for x in [21, 80, 81, 300, 213, 1100, 1201, 301000, 80080]:\n",
    "    print(\"{} {}\".format(x,to_french_phrase(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "...und erzeugen dann trainings und testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers, french_numbers = generate_translations(\n",
    "    low=1, high=int(1e6) - 1, exhaustive=5000, random_seed=0)\n",
    "num_trn, num_dev, fr_trn, fr_dev = train_test_split()\n",
    "\n",
    "num_val, num_tst, fr_val, fr_tst = train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq mit LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir definieren nun ein sequence-to-sequence Modell mit Hilfe eines LSTMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"basic_seq2seq.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"basic_seq2seq.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunaechst wandeln wir eingabe und ausgabe in listen um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_input_output(source_tokens, target_tokens):\n",
    "    input_tokens = source_tokens + ['_GO'] + target_tokens\n",
    "    output_tokens = target_tokens + ['_EOS']\n",
    "    return input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun fuegen wir Input und Output zusammen und fitten einen Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "[pairs_trn, pairs_val, pairs_tst] = [None, None, None]\n",
    "tokenizer.fit_on_texts(pairs_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'deux mille huit cent quatre vingt deux _GO 2 8 8 2'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_trn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir definieren ausserdem ein dictionary um token-ids in woerter zurueckzuverwandeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.word_index\n",
    "idx2word = dict({})\n",
    "idx2word[0] = ''\n",
    "idx2word[40] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Trainingsdaten werden mithilfe dieses Tokenizers transformiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[X_trn, X_val, X_tst] = []\n",
    "[Y_trn, Y_val] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 3, 19, 1, 4, 14, 17, 2, 6, 9, 9, 6]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 9, 9, 6, 40]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_trn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschliessend forcieren wir alle Daten auf die gleiche Laenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[X_trn, X_val, X_tst, Y_trn, Y_val] = [sequence.pad_sequences() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0, 17,  3, 19,  1,  4, 14, 17,  2,  6,\n",
       "        9,  9,  6], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trn[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Texterzeugung verwenden wir ein LSTM das auf Embeddings basiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_seq2seq = Sequential([\n",
    "    Embedding(None, None, input_length = None, mask_zero = True),\n",
    "    LSTM(),\n",
    "    #Dropout(0.2),\n",
    "    Dense()\n",
    "]\n",
    ")\n",
    "\n",
    "simple_seq2seq.compile(loss = 'sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Modell trainieren wir nun auf den tokenisierten Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_seq2seq.optimizer.lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 40s - loss: 1.1158 - val_loss: 0.8558\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 41s - loss: 0.7496 - val_loss: 0.6489\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 41s - loss: 0.4751 - val_loss: 0.3737\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 42s - loss: 0.2750 - val_loss: 0.2242\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 40s - loss: 0.1716 - val_loss: 0.1691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.keras.python.keras.callbacks.History at 0x7f502daf4f28>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_seq2seq.fit(X_trn, np.expand_dims(Y_trn, -1),\n",
    "                             validation_data = (X_val, np.expand_dims(Y_val, -1)),\n",
    "                             epochs = 5, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir speichern das gefittete Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_seq2seq.save_weights('../models/french_num_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_seq2seq.load_weights('../../refereeReports/talks/deepLearning/models/french_num_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt pruefen wir das Modell anhand einiger Beispiele. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction token ids: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  6 13 12 11 15 40]\n",
      "predicted number: 329750\n",
      "test number: 329750\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction token ids:\", None)\n",
    "print(\"predicted number:\", None)\n",
    "print(\"test number:\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nun fuehren wir eine Greedy-Uebersetzung ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy_translate(model, source_sequence):\n",
    "    \"\"\"Greedy decoder recursively predicting one token at a time\"\"\"\n",
    "    input_ids = tokenizer.texts_to_sequences([source_sequence])[0] + [2]\n",
    "\n",
    "    # Prepare a fixed size numpy array that matches the expected input\n",
    "    # shape for the model\n",
    "    input_array = np.zeros(shape=(1, model.input_shape[1]),\n",
    "                           dtype=np.int32)\n",
    "    decoded_tokens = []\n",
    "    while len(input_ids) <= MAXLEN:\n",
    "        #update input_array\n",
    "        \n",
    "        # Predict the next output: greedy decoding with argmax\n",
    "        next_token_id = None\n",
    "        \n",
    "        # Stop decoding if the network predicts end of sentence:\n",
    "        if next_token_id in [0, 40]:\n",
    "            break\n",
    "            \n",
    "        # Otherwise append output to decoded tokens and input_ids\n",
    "\n",
    "        \n",
    "    return ''.join([None for token_id in decoded_tokens])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir testen die Greedy-Uebersetzung an Beispielsaetzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un: 0\n",
      "deux: 20\n",
      "trois: 3\n",
      "onze: 100\n",
      "quinze: 15\n",
      "cent trente deux: 132\n",
      "cent mille douze: 10102\n",
      "sept mille huit cent cinquante neuf: 7859\n",
      "vingt et un: 21\n",
      "vingt quatre: 24\n",
      "quatre vingts: 80\n",
      "quatre vingt onze mille: 911\n",
      "quatre vingt onze mille deux cent deux: 91202\n"
     ]
    }
   ],
   "source": [
    "phrases = [\n",
    "    \"un\",\n",
    "    \"deux\",\n",
    "    \"trois\",\n",
    "    \"onze\",\n",
    "    \"quinze\",\n",
    "    \"cent trente deux\",\n",
    "    \"cent mille douze\",\n",
    "    \"sept mille huit cent cinquante neuf\",\n",
    "    \"vingt et un\",\n",
    "    \"vingt quatre\",\n",
    "    \"quatre vingts\",\n",
    "    \"quatre vingt onze mille\",\n",
    "    \"quatre vingt onze mille deux cent deux\",\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = greedy_translate(simple_seq2seq, phrase)\n",
    "    print('{}: {}'.format(phrase, ''.join(translation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellauswertung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun werten wir die Qualitaet des Modells aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_accuracy(model, num_sequences, fr_sequences, n_samples=300,\n",
    "                    decoder_func=greedy_translate):\n",
    "    correct = 0\n",
    "    return np.mean([num_seq == decoder_func(simple_seq2seq, fr_seq) \n",
    "                    for _, num_seq, fr_seq in zip(range(n_samples), num_sequences, fr_sequences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94666666666666666"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_accuracy(simple_seq2seq, num_tst, fr_tst, n_samples = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schliesslich implementieren wir noch beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/beam_search.py\n",
    "def beam_translate(model, source_sequence,\n",
    "                   word_level_source=True, word_level_target=True,\n",
    "                   beam_size=10, return_ll=False):\n",
    "    \"\"\"Decode candidate translations with a beam search strategy\n",
    "    \n",
    "    If return_ll is False, only the best candidate string is returned.\n",
    "    If return_ll is True, all the candidate strings and their loglikelihoods\n",
    "    are returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare a fixed size numpy array that matches the expected input\n",
    "    # shape for the model\n",
    "    input_array = np.empty(shape=(beam_size, model.input_shape[1]),\n",
    "                           dtype=np.int32)\n",
    "    input_ids = tokenizer.texts_to_sequences([source_sequence])[0] + [2]\n",
    "    \n",
    "    \n",
    "    # initialize loglikelihood, input token ids, decoded tokens for\n",
    "    # each candidate in the beam\n",
    "    candidates = [(0, input_ids[:], [], False)]\n",
    "    \n",
    "    while any([not done and (len(input_ids) < MAXLEN)\n",
    "               for _, input_ids, _, done in candidates]):\n",
    "        # Vectorize a the list of input tokens and use zeros padding.\n",
    "        input_array.fill(0)\n",
    "        for i, (_, input_ids, _, done) in enumerate(candidates):\n",
    "            if not done:\n",
    "                input_array[i, -len(input_ids):] = input_ids\n",
    "        \n",
    "        # Predict the next output in a single call to the model to amortize\n",
    "        # the overhead and benefit from vector data parallelism on GPU.\n",
    "        next_likelihood_batch = model.predict(input_array)\n",
    "        \n",
    "        # Build the new candidates list by summing the loglikelood of the\n",
    "        # next token with their parents for each new possible expansion.\n",
    "        new_candidates = []\n",
    "        for i, (ll, input_ids, decoded, done) in enumerate(candidates):\n",
    "            if done:\n",
    "                new_candidates.append((ll, input_ids, decoded, done))\n",
    "            else:\n",
    "                next_loglikelihoods = np.log(next_likelihood_batch[i, -1])\n",
    "                for next_token_id, next_ll in enumerate(next_loglikelihoods):\n",
    "                    new_ll = ll + next_ll\n",
    "                    new_input_ids = input_ids[:]\n",
    "                    new_input_ids.append(next_token_id)\n",
    "                    new_decoded = decoded[:]\n",
    "                    new_done = done\n",
    "                    if next_token_id  in [0, 40]:\n",
    "                        new_done = True\n",
    "                    if not new_done:\n",
    "                        new_decoded.append(next_token_id)\n",
    "                    new_candidates.append(\n",
    "                        (new_ll, new_input_ids, new_decoded, new_done))\n",
    "        \n",
    "        # Only keep a beam of the most promising candidates\n",
    "        new_candidates.sort(reverse=True)\n",
    "        candidates = new_candidates[:beam_size]\n",
    "\n",
    "    separator = \" \" if word_level_target else \"\"\n",
    "    if return_ll:\n",
    "        return [(separator.join(decoded), ll) for ll, _, decoded, _ in candidates]\n",
    "    else:\n",
    "        _, _, decoded, done = candidates[0]\n",
    "        return ''.join([idx2word[token_id] for token_id in decoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un: \n",
      "deux: 20\n",
      "trois: 3\n",
      "onze: 11\n",
      "quinze: 15\n",
      "cent trente deux: 132\n",
      "cent mille douze: 10102\n",
      "sept mille huit cent cinquante neuf: 7859\n",
      "vingt et un: 21\n",
      "vingt quatre: 24\n",
      "quatre vingts: 80\n",
      "quatre vingt onze mille: 911\n",
      "quatre vingt onze mille deux cent deux: 91202\n"
     ]
    }
   ],
   "source": [
    "phrases = [\n",
    "    \"un\",\n",
    "    \"deux\",\n",
    "    \"trois\",\n",
    "    \"onze\",\n",
    "    \"quinze\",\n",
    "    \"cent trente deux\",\n",
    "    \"cent mille douze\",\n",
    "    \"sept mille huit cent cinquante neuf\",\n",
    "    \"vingt et un\",\n",
    "    \"vingt quatre\",\n",
    "    \"quatre vingts\",\n",
    "    \"quatre vingt onze mille\",\n",
    "    \"quatre vingt onze mille deux cent deux\",\n",
    "]\n",
    "for phrase in phrases:\n",
    "    translation = beam_translate(simple_seq2seq, phrase)\n",
    "    print('{}: {}'.format(phrase, translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
